{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import HF_LLMQuery, OpenAI_LLMQuery\n",
    "import openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Easiest way to generate SQL query from text with zero-shot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using HF Model for SQL Queries\n",
    "\n",
    "First we should specify the configs paths. By default there are some, you can use them at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = \"configs/models/hf_model_config.yaml\"\n",
    "PROMPT_CONFIG = \"configs/prompts/Zero_shot.yaml\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we simply query with our question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 active agency customers on january 1st, 2022\n"
     ]
    }
   ],
   "source": [
    "LLM_Simple = HF_LLMQuery(MODEL_CONFIG, PROMPT_CONFIG)\n",
    "\n",
    "query = \"How many active agency customers did we have on January 1st, 2022?\"\n",
    "\n",
    "ans = LLM_Simple.query(query)\n",
    "print(ans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ChatGPT for SQL Queries\n",
    "\n",
    "Now we specify configs for ChatGPT. Default could be used as well, but it is crucial to provide openai token and write it to the model configs as follows:\n",
    "\n",
    "```yaml\n",
    "token: sk1824...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = \"configs/models/openai_model_config.yaml\"\n",
    "PROMPT_CONFIG = \"configs/prompts/Zero_shot.yaml\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can do exatly the same thing as it was before. \n",
    "\n",
    "Sometimes there could be problems with OpenAI API so the token could be invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Token\n"
     ]
    }
   ],
   "source": [
    "ChatGPT = OpenAI_LLMQuery(MODEL_CONFIG, PROMPT_CONFIG)\n",
    "query = \"How many active agency customers did we have on January 1st, 2022?\"\n",
    "\n",
    "try:\n",
    "    ans = ChatGPT.query(query)\n",
    "    print(ans)\n",
    "except (openai.error.AuthenticationError, openai.error.RateLimitError) as e:\n",
    "    print(\"Invalid Token\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using one-shot\n",
    "\n",
    "To enable one-shot inference we need to change configs, so the text for few-shot is present there.\n",
    "\n",
    "In every config one can find a field named \"few_shot_text\". If the text is passed in it, it will be used as a few_shot. \n",
    "\n",
    "In the example before we will use another configs, that has few-shot text like it is shown below.\n",
    "\n",
    "\n",
    "```yaml\n",
    "  few_shot_text: \"question: get people name with age equal 25 table: id, name, age \\n SELECT name FROM table WHERE age = 25\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = (\n",
    "    \"configs/models/hf_model_config.yaml\"  # here we could have used OpenAI model\n",
    ")\n",
    "PROMPT_CONFIG = \"configs/prompts/One_shot.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT active agency FROM table WHERE date = january 1st, 2022\n"
     ]
    }
   ],
   "source": [
    "LLM_Simple = HF_LLMQuery(MODEL_CONFIG, PROMPT_CONFIG)\n",
    "\n",
    "query = \"How many active agency customers did we have on January 1st, 2022?\"\n",
    "\n",
    "ans = LLM_Simple.query(query)\n",
    "print(ans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing table and columns names in a prompt\n",
    "\n",
    "It is usually a great technique to boost models quality. A lot of HuggingFace models as well expect to see those names to work better.\n",
    "\n",
    "To do so, there is Prompt Schema that works with this case. To use it, we will need to specify it in configs in a very simply manner as shown below.\n",
    "\n",
    "```yaml\n",
    "    PromptClass: QuestionTableRowsPrompt\n",
    "```\n",
    "\n",
    "and if we want to have the standart prompting, it should be \n",
    "\n",
    "```yaml\n",
    "    PromptClass: SimplePrompt\n",
    "```\n",
    "\n",
    "## Adding model instructions\n",
    "\n",
    "Notice that we can pass a parameter of instruction, that will serve as \"system\" message. That could possibly increase generation quality for Large models with emergent abilities.\n",
    "\n",
    "To pass the instruction we need to change config as well and add a text for an instruction\n",
    "\n",
    "```yaml\n",
    "    prompt_configs:\n",
    "        instruction_text: \"Act as a professional SQL developer and answer a question with a step by step reasoning\"\n",
    "```\n",
    "\n",
    "## Using Question Decomposition to improve performance\n",
    "\n",
    "As shown in an [article](https://arxiv.org/pdf/2305.14215.pdf) decomposition of the question could help the model to solve more complex task. To do so, lets change our one-shot example to suit the suggested methodology.\n",
    "\n",
    "Now our one-shot prompt is\n",
    "\n",
    "```yaml\n",
    "    prompt_configs:\n",
    "          few_shot_text: |\n",
    "            question: get people name with age equal 25 table: id, name, age\n",
    "            First need to select all the people from table\n",
    "            SELECT * FROM table\n",
    "            Second need to select only people with age equal 25, that corresponds to the column age\n",
    "            SELECT * FROM table WHERE age = 25\n",
    "            A: SELECT * FROM table WHERE age = 25\n",
    "```\n",
    "\n",
    "#### Taking everything together \n",
    "\n",
    "We now will have the following config\n",
    "\n",
    "``` yaml\n",
    "PromptClass: QuestionTableColumnsPrompt\n",
    "prompt_configs:\n",
    "  few_shot_text: |\n",
    "    question: get people name with age equal 25 table: id, name, age\n",
    "    First need to select all the people from table\n",
    "    SELECT * FROM table\n",
    "    Second need to select only people with age equal 25, that corresponds to the column age\n",
    "    SELECT * FROM table WHERE age = 25\n",
    "    A: SELECT * FROM table WHERE age = 25\n",
    "  instruction_text: \"Act as a professional SQL developer and answer a question with a step by step reasoning\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: SELECT COUNT customer_action FROM table WHERE date = january 1st, 2022\n"
     ]
    }
   ],
   "source": [
    "MODEL_CONFIG = (\n",
    "    \"configs/models/hf_model_config.yaml\"  # here we could have used OpenAI model\n",
    ")\n",
    "PROMPT_CONFIG = \"configs/prompts/QDecomp.yaml\"\n",
    "\n",
    "LLM_QDecomp = HF_LLMQuery(MODEL_CONFIG, PROMPT_CONFIG)\n",
    "\n",
    "query = \"How many active agency customers did we have on January 1st, 2022?\"\n",
    "tables = \"my_table\"\n",
    "columns = [\"id\", \"customer_action\", \"date\"]\n",
    "\n",
    "ans = LLM_QDecomp.query(query, tables=tables, columns=columns)\n",
    "print(ans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "\n",
    "#### Changing Models\n",
    "\n",
    "Changing models is a very simply process. We should just change the variable inside the config, that downloads it from HuggingFace and the class name that it uses.\n",
    "\n",
    "For example, we would like to take just GPT-2:\n",
    "\n",
    "```yaml\n",
    "TokenizerClass: GPT2Tokenizer\n",
    "ModelClass: GPT2Model\n",
    "\n",
    "model:\n",
    "  pretrained_model_name_or_path: gpt2\n",
    "tokenizer:\n",
    "  pretrained_model_name_or_path: gpt2\n",
    "```\n",
    "\n",
    "#### Changing models generation arguments\n",
    "\n",
    "The same happens with the generation arguments. One can change the config file and add any relevant arguments for generation.\n",
    "\n",
    "```yaml\n",
    "    generation_args:\n",
    "    max_new_tokens: 32\n",
    "    num_beams: 12\n",
    "```\n",
    "\n",
    "\n",
    "#### Custom Prompt Schemas\n",
    "\n",
    "\n",
    "There could be cases when user would like to create his own type of prompt, for example, it will contain specification of database as input, or something else.\n",
    "\n",
    "Therefore, to make new prompt style, one should visit ```prompting/hf_prompt_schemas.py``` or ```prompting/openai_prompt_schemas.py```. THere are differencies due to the API specifics. Then, one should simply implement custom Prompt Schema with a method ```__call_``` and that inherits ```SimplePrompt``` and uses ```_transforms``` method from super class before return. \n",
    "\n",
    "For example, we want to pass the name of database\n",
    "\n",
    "```python\n",
    "\n",
    "class DB_NamePrompt(SimplePrompt):\n",
    "    def __call__(self, query, db_name, **kwargs):\n",
    "        updated_query = \"{}, dbname: {}\".format(query, db_name)\n",
    "        return self._transforms(updated_query)\n",
    "```\n",
    "\n",
    "Thats it! \n",
    "\n",
    "Now if we would like to use this prompt, we simply should change our config to the following:\n",
    "\n",
    "```yaml\n",
    "    PromptClass: DB_NamePrompt\n",
    "```\n",
    "\n",
    "\n",
    "#### Query and execute the same time\n",
    "\n",
    "There may be a need to test the functionality of model, so we would like to execute query immediately when the model output was obtained, for example when calculating metrics for Text2SQL Task. To do so we should provide path to the database credentials and after call the ```query_and_execute()``` method just the same way we did before with just query.\n",
    "\n",
    "Notice, that the time of execution is limited, you can manually change it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
